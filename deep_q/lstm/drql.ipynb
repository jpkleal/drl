{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from deep_q.commons import DeepQConfig\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "Run = namedtuple(\"Run\", ('states', 'actions', 'rewards'))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Run(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(list(self.memory), batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f68baef596f5f6a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DRQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=128, hidden_layers=1):\n",
    "        super(DRQN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, hidden_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "      \n",
    "    def forward(self, x, i=None):\n",
    "        if i is not None:\n",
    "            x, i = self.lstm(x, i)\n",
    "        else:\n",
    "            x, i = self.lstm(x)\n",
    "        return self.out(x), i\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e730898a6245467"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DRQN_Agent:\n",
    "    def __init__(self, net:nn.Module, config:DeepQConfig, path=None):\n",
    "        self.config = config\n",
    "\n",
    "        self.target_net = net(self.config.n_inputs, self.config.n_outputs, **self.config.net_kwargs).to(self.config.device)\n",
    "\n",
    "        if path:\n",
    "            self.q_net = torch.load(path).to(self.config.device)\n",
    "        else:\n",
    "            self.q_net = net(self.config.n_inputs, self.config.n_outputs, **self.config.net_kwargs)\n",
    "\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.config.lr)\n",
    "        self.memory = ReplayMemory(self.config.rm_size)\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        \n",
    "    def soft_update_target_net(self):\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.q_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * self.config.tau + target_net_state_dict[key] * (\n",
    "                    1 - self.config.tau)\n",
    "        self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "    def add_to_memory(self, *args):\n",
    "        self.memory.push(*args)\n",
    "\n",
    "    def select_action(self, state, i=None, epsilon=0):\n",
    "        with torch.no_grad():\n",
    "            x, i = self.q_net(state.view(-1, self.config.n_inputs), i)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            # Explore: take a random action\n",
    "            return torch.tensor([random.randrange(self.config.n_outputs)], device=self.config.device, dtype=torch.long), i\n",
    "        else:\n",
    "            # Exploit: select the highest Q value\n",
    "            return x.max(1)[1][-1].view(1), i\n",
    "\n",
    "    def Q(self, state, action):\n",
    "        state = pad_sequence(state, batch_first=True)\n",
    "        action = pad_sequence(action, batch_first=True)\n",
    "\n",
    "        x, _= self.q_net(state)\n",
    "        return x.gather(2, action.view(self.config.batch_size, -1, 1))\n",
    "\n",
    "    def target(self, state, reward):\n",
    "        state = pad_sequence(state, batch_first=True)\n",
    "        reward = pad_sequence(reward, batch_first=True)\n",
    "\n",
    "        x, _ = self.target_net(state)\n",
    "        Q_target = x.max(2)[0].detach()\n",
    "\n",
    "        \n",
    "        return (Q_target * self.config.gamma) + reward\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.memory) < self.config.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = Run(*zip(*self.memory.sample(self.config.batch_size)))\n",
    "\n",
    "        next_state_batch = (i[3:] for i in batch.states)\n",
    "        state_batch = (i[:-3] for i in batch.states)\n",
    "        action_batch = batch.actions\n",
    "        reward_batch = batch.rewards\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # state_batch.shape = (N x L x states)\n",
    "        # action_batch.shape = (N x L x 1)\n",
    "        y = self.Q(state_batch, action_batch).view(self.config.batch_size, -1)\n",
    "\n",
    "        # next_state_batch.shape = (N x L x states)\n",
    "        # reward_batch.shape = (N x L x 1)\n",
    "        yl = self.target(next_state_batch, reward_batch).view(self.config.batch_size, -1)\n",
    "\n",
    "        loss = self.criterion(y, yl)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.cpu().data.item()\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.q_net, name)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5342a9006305964e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
